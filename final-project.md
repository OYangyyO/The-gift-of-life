

## Pixelated Echo

**Student Name (Id#)**: Xiwen Yang (24010647)

**Sketch Link**: https://git.arts.ac.uk/pages/24010647/Pixelated-Echo-line/

**Source Code**: https://editor.p5js.org/3190601180/sketches/v-IPlli21

**Project Video**: [Pixelated Echo video](https://youtu.be/ubdQsHe2ta8)  

**References**: 

*fixed pixelation* ([link](https://openprocessing.org/sketch/2456109)) 

*Web Speech API* ([link](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API)) 


## Pixelated Echo – Project Description 

In today’s fast-paced digital world, we consume information **faster than ever**—scrolling through headlines, watching short clips, and letting AI filter what we see. But what do we lose when we **prioritize speed over depth**? *Pixelated Echo* is an **interactive generative art project** that explores this question. By using **real-time speech recognition and dynamic pixelation**, the work visualizes what happens when communication is reduced to **only keywords**, stripping away depth, context, and ultimately, self-expression.

![44cb2f3d-3f18-4a67-89bb-e0d32a33e372](https://github.com/user-attachments/assets/c41cfa55-33e0-4ae6-9dd8-56567e61b1a9)

As users speak, the system **extracts only key terms**, and the fewer words retained, the **more their image becomes pixelated**. This serves as a metaphor for **how filtering in modern communication erases details and identity**. The project creates a direct, interactive experience where users physically **see themselves disappear** as their speech is reduced.  

The piece was developed using **Web Speech API** for real-time speech recognition, **Natural Language Processing (NLP)** to extract keywords, and **p5.js** for pixelation and video processing. The system links **speech complexity to image clarity**, creating a **fluid and responsive** interaction. 

![4e61de0a-93a0-4741-b7bf-7c482021c423](https://github.com/user-attachments/assets/77e96d93-4762-4608-82b0-2b8b92041358)

Inspired by an OpenProcessing reference project on **fixed pixelation** ([link](https://openprocessing.org/sketch/2456109)), I modified it to make **pixelation dynamic and real-time**, tied directly to the user’s spoken input.  

![0649f5f2-0c3f-490b-8ede-c6a0a00df5b3](https://github.com/user-attachments/assets/edc4d826-5afb-40aa-a061-478e53377ba3)

The inspiration for my project *Pixelated Echo* comes from **Mario Klingemann’s "Appropriate Response" (2020)**, which examines how filtering alters meaning in communication. Three key ideas from his work influenced me:

![b60e7d3d-8efc-40ce-913f-0cc35be992f4](https://github.com/user-attachments/assets/c7c9409c-2d00-48a5-8410-2b2f03fa9a8e)

1. **Information Reduction** – The more we cut down, the more meaning we lose.  
2. **The Visual Representation of Loss** – Missing details are not just felt but seen.  
3. **Dehumanization of Expression** – Over-filtering turns communication into mechanical, fragmented pieces.  

These ideas resonate with **how we engage with media today**. We skim instead of read, we extract instead of understand, and we let **algorithms decide what we consume**. Social media platforms and AI-driven recommendations **shape our thoughts by choosing what stays and what disappears**. *Pixelated Echo* turns this process into an immersive experience—letting users **feel the effects of information loss in real time**.  

Moving forward, I plan to expand the project by allowing users to **control the level of filtering**, making them **actively decide how much information to keep or lose**. Additionally, adding **sound distortion** would enhance the experience, making **expression loss audible as well as visual**.  

At its core, *Pixelated Echo* is not just an experiment in **real-time interaction and generative art**—it is a **critical reflection on the cost of efficiency in modern communication**. By turning **information filtering into a tangible experience**, the project asks: **As we strip away meaning for the sake of speed, what’s left of us?**  

## Presentation feedback

During my presentation, my professor pointed out that **users might feel nervous and struggle to express deep thoughts**, which could limit the impact of my project. To address this, I plan to **introduce thought-provoking questions** before users speak, guiding them toward **more meaningful responses**.  

This change will make the filtering effect **more powerful**, as it will now target **deep personal expressions**, reinforcing the idea that **reducing speech also reduces individuality**. As key words disappear, **the person’s image will gradually pixelate**.

This refinement strengthens the project’s **critical perspective**, making the loss of self **more visible and emotionally impactful**. Moving forward, I will adjust **speech recognition and visual processing** to enhance this effect and deepen the audience’s engagement.

## Future Outlook

On a technical level, I'll be focusing more on how to get the system to determine what the right keywords are, as it still has problems recognising keywords. This will be combined with a deeper update of the suggestions given to me by the professor. I think the critical relationship between personal identity and information efficiency will also be very visualised for the audience through my project.

## Project Images

![4cf4f07b-ce3f-4222-809d-65d72a6bca47](https://github.com/user-attachments/assets/35cf8ffb-bf72-43e8-8842-d182223fa498)

![fe8d857b-edee-4852-97ce-f52f23ccd32a](https://github.com/user-attachments/assets/d0115937-1008-419e-893e-55e98cf6522e)


